# Audio Processing Improvements for Podcast Transcriber
# Apply these changes to main.py for better sentence handling

## 1. Add smart audio buffering settings to __init__ method (around line 50):

Add after: self.current_sentence = ""  # Current sentence being transcribed

# Smart audio buffering settings
self.audio_buffer = []
self.silence_detection_threshold = 0.01  # Volume threshold for silence
self.silence_duration = 0  # Track silence duration in seconds
self.min_silence_for_send = 1.5  # Send after 1.5 seconds of silence
self.max_buffer_duration = 8  # Maximum 8 seconds before forced send
self.last_audio_time = 0
self.chunk_duration = 0.1  # Process in 100ms chunks for responsiveness
self.last_processing_time = 0

## 2. Update start_listening method (around line 72):

Add after: self.status_updated.emit("Listening...")

# Reset audio buffering state
self.audio_buffer = []
self.silence_duration = 0
self.last_audio_time = 0
self.last_processing_time = 0

## 3. Update stop_listening method (around line 83):

Replace the entire stop_listening method with:

def stop_listening(self):
    self.is_listening = False
    # Process any remaining audio in buffer
    if self.audio_buffer and len(self.audio_buffer) > 16000:  # At least 1 second
        print("Processing remaining audio on stop...")
        self._process_audio_chunk(self.audio_buffer, 16000)
    
    # If there's a current sentence, add it as a completed sentence
    if self.current_sentence and self.current_sentence.strip():
        self.sentences.append(self.current_sentence.strip())
        self.transcription_updated.emit(self.current_sentence.strip())
        self.current_sentence = ""
    self.status_updated.emit("Ready to listen")

## 4. Replace the _record_audio method (around line 92):

Replace the entire _record_audio method with the improved version that uses:
- 100ms chunks instead of 3-second chunks
- Smart silence detection
- Context preservation (keeps last 2 seconds)

## 5. Add new _should_process_audio method:

Add this new method after _record_audio:

def _should_process_audio(self, current_time):
    """Smart logic to determine when to send audio for transcription"""
    if not self.audio_buffer:
        return False
        
    # Calculate audio volume (RMS) from last 0.5 seconds
    recent_samples = int(16000 * 0.5)  # 0.5 seconds
    audio_array = np.array(self.audio_buffer[-recent_samples:]) if len(self.audio_buffer) >= recent_samples else np.array(self.audio_buffer)
    
    if len(audio_array) == 0:
        return False
        
    rms_volume = np.sqrt(np.mean(audio_array**2))
    
    # Check for silence
    is_silent = rms_volume < self.silence_detection_threshold
    
    if is_silent:
        self.silence_duration += self.chunk_duration
    else:
        self.silence_duration = 0
        self.last_audio_time = current_time
    
    # Decision logic:
    # 1. Send if we've had silence for min_silence_for_send seconds
    if self.silence_duration >= self.min_silence_for_send:
        print(f"Sending due to silence: {self.silence_duration:.1f}s")
        return True
        
    # 2. Send if buffer is getting too long (max_buffer_duration)
    buffer_duration = len(self.audio_buffer) / 16000
    if buffer_duration >= self.max_buffer_duration:
        print(f"Sending due to max buffer duration: {buffer_duration:.1f}s")
        return True
        
    # 3. Send if we've been recording for a while without processing
    time_since_last_processing = current_time - self.last_processing_time
    if time_since_last_processing >= 5.0:  # 5 seconds without processing
        print(f"Sending due to time since last processing: {time_since_last_processing:.1f}s")
        return True
        
    return False

## 6. Update _process_audio_chunk method:

Update the method to use the new audio_buffer instead of the old audio_buffer parameter.

## Key Improvements:

✅ **Smart Silence Detection**: Waits for 1.5s of silence before sending
✅ **Maximum Buffer Duration**: Won't wait longer than 8 seconds
✅ **Safety Net**: Sends after 5s without processing
✅ **Context Preservation**: Keeps last 2 seconds for better accuracy
✅ **Small Chunks**: Processes in 100ms chunks for responsiveness
✅ **Complete Sentences**: Reduces sentence breaking significantly

## Expected Results:

- Sentences will be much more complete
- Fewer API calls (more efficient)
- Better transcription accuracy
- More natural speech flow
- Reduced sentence fragmentation
